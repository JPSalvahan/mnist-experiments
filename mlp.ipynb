{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 images.\n",
      "Loaded 20000 images.\n",
      "Loaded 30000 images.\n",
      "Loaded 40000 images.\n",
      "Loaded 50000 images.\n",
      "Loaded 60000 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import io, transforms\n",
    "\n",
    "# List of tuples (image, label)\n",
    "dataset = []\n",
    "\n",
    "# Load images and labels from files\n",
    "with open(\"dataset\\labels.txt\", \"r\") as labels:\n",
    "    for i, line in enumerate(labels):\n",
    "        img_path = os.path.join('dataset\\images', f\"{i}.png\")\n",
    "        img = io.read_image(img_path) / 255.0\n",
    "        dataset.append((img, int(line.strip())))\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print(f\"Loaded {i+1} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37242.png\n",
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tjQfCuveJ5jFoulXN4QwVnjXCIT03OcKv4kVT1XS73RNUuNN1GAwXlu+yWMsDtP1BIP4VTor1r4ha/rGi6d4e8K6Glzp2nPpcEhW3G03jyAFjkcnnj3JOe1YXjLwvH4a8GeH5dSt5z4h1NnuZZXlJ2QgALGQT97lT7dK4Kiuw0P4heL9PtbPSNM1MBUJitfMhjd4t/GEdlJUc9jxWt8b76W5+I81pJcGdbC2ht1ctkk7AzE9s7mOcYrzminI7xSLJGzI6kMrKcEEdCDV3WNa1HX9RfUNVu5Lq7cANLJ1wBgDjpVCv//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4AWNgGLrArfL/3yjszo//8P3P3z/zEJIscKbkchMOhnPTPwrDRZAYm/+8XxOPVYaBweHjnw0QpQ4n5kL1wI0V5P7aDxHTNfkHlYRTgX82Qtj8tz8mwkWhjMA/ThBWzp+dMDkmGIOBwZ8VxLZvRYggSWZ7AYXbKrgRkghW598XlQ1////9+9cUIQhjsbT+AYJJ+/5sYIYJIdFsUkDAkP1nDZIYGvP2Hxu4CJKDIGKiDK9xSkohq0Zmg3TEc338hVMnA8PuJzglf/4PyYRLYjBWtuCIUwyV+AQA1yBABqJVYAsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample from the set\n",
    "rand_i = torch.randint(0, len(dataset), (1,)).item()\n",
    "print(f\"{rand_i}.png\")\n",
    "print(f\"Label: {dataset[rand_i][1]}\")\n",
    "display(transforms.ToPILImage()(dataset[rand_i][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
